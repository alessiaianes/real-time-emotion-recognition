{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the necessary **libraries**\n",
    "* **OPENCV** - used to manage **video capture and optical flow**\n",
    "* **MTCNN** - used to detect **faces**\n",
    "* **DEEPFACE** - used to detect **facial expressions**\n",
    "* **NUMPY** - used to manage the **visualization** of the output (in this project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # OpenCV - to manage video capture and optical flow\n",
    "from mtcnn import MTCNN # MTCNN - to detect faces\n",
    "from deepface import DeepFace # DeepFace - to detect facial expressions\n",
    "import numpy as np # Numpy - here used to mangae the visualization of the output\n",
    "import imutils # Imutils - to resize the frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to draw **dots and arrows** (for the optical flow)\n",
    "* **drawDotsandArrows** - draw dots on each block and arrows where there is a significant movement (used for optical flow)\n",
    "    + `frame_displayed`: frame on which arrows and dots will be drawn\n",
    "    + `flow`: optical flow of the frame\n",
    "    + `blockSize`: size of the blocks (on which dots will be drawn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawDotsAndArrows(frame_displayed, flow, blockSize): # Draw dots on each block and arrows where there is movement\n",
    "    h, w = frame_displayed.shape[:2]\n",
    "    step = blockSize\n",
    "    scale = 5  # Length of the arrow\n",
    "    threshold = 1  # Filters minor movements\n",
    "    frame_displayed = cv2.cvtColor(frame_displayed, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            fx, fy = flow[y, x] # Movement vector for each block (x,y)\n",
    "            # Draw a white dot on each block\n",
    "            cv2.circle(frame_displayed, (x, y), 1, (255, 0, 0), -1)\n",
    "            # Draw a green arrow if the movement is significant\n",
    "            if abs(fx) > threshold or abs(fy) > threshold:\n",
    "                end_x = int(x + fx * scale) \n",
    "                end_y = int(y + fy * scale) \n",
    "                cv2.arrowedLine(frame_displayed, (x, y), (end_x, end_y), (0, 255, 0), 1, tipLength=0.5)\n",
    "\n",
    "    return frame_displayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial and emotion **recognition** and computation of the **optical flow**\n",
    "\n",
    "1. **Initialization** of\n",
    "    1. **Variables for optical flow**\n",
    "    2. **Webcam**\n",
    "    3. MTCNN **detector**\n",
    "    4. Variables for **emotion/facial recognition**\n",
    "    5. Dictionary for emotion's **colors**\n",
    "2. Computation of **optical flow**\n",
    "3. Recognition of **faces and emotions**\n",
    "4. **Visualization** of emotions detected and optical flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables for optical flow\n",
    "displayHeight = 480\n",
    "blockSize = 16\n",
    "\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "\n",
    "# Read the first frame\n",
    "ret, prev_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: No frame acquired.\")\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "# Initialize the MTCNN detector\n",
    "detector = MTCNN()\n",
    "\n",
    "# Initialize variables to manage the emotion analysis\n",
    "frame_count = 0\n",
    "frame_interval = 10 # Do the analysis every 10 frames\n",
    "\n",
    "# Variables to store the data of the faces detected\n",
    "face_data = []\n",
    "\n",
    "# Dictionary for the colors associated with each emotion\n",
    "emotion_colors = {\n",
    "    'angry': (0, 0, 255),       # Red\n",
    "    'disgust': (0, 255, 0),     # Green\n",
    "    'fear': (137, 49, 66),      # Purple\n",
    "    'happy': (72, 205, 255),    # Yellow\n",
    "    'sad': (255, 127, 0),       # Light Blue\n",
    "    'surprise': (255, 0, 255),  # Yellow\n",
    "    'neutral': (128, 128, 128)  # Gray\n",
    "}\n",
    "\n",
    "# Resize the first frame captured previously\n",
    "prev_frame = cv2.flip(imutils.resize(prev_frame, displayHeight),1)\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Acquire a new frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Check if the frame was acquired correctly, otherwise exit the loop and return an error message\n",
    "    if not ret:\n",
    "        print(\"Errore nell'acquisizione del frame.\")\n",
    "        break\n",
    "\n",
    "\n",
    "    #Computation of the optical flow\n",
    "\n",
    "    # Resize the frame and convert it into a grayscale\n",
    "    frame = cv2.flip(imutils.resize(frame, displayHeight),1)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    # Compute the optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, \n",
    "                                        pyr_scale=0.5, levels=3, \n",
    "                                        winsize=25, iterations=3, \n",
    "                                        poly_n=5, poly_sigma=1.2, flags=0)\n",
    "\n",
    "    # Update the previous frame \n",
    "    prev_gray = gray.copy()\n",
    "    \n",
    "    # Draw dots and arrows on the frame\n",
    "    flow_frame = drawDotsAndArrows(prev_gray, flow, blockSize)\n",
    "\n",
    "    # Convert the frame into RGB (DeepFace uses RGB, OpenCV uses BGR)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform facial detection and emotion recognition every 10 frames\n",
    "    if frame_count % frame_interval == 0:\n",
    "        face_data = []\n",
    "\n",
    "        # Face detection with MTCNN\n",
    "        faces = detector.detect_faces(rgb_frame)\n",
    "\n",
    "        for face in faces:\n",
    "            # Extract face's coordinates\n",
    "            x, y, w, h = face['box']\n",
    "            \n",
    "            # Crop the face from the frame\n",
    "            face_img = rgb_frame[y:y + h, x:x + w]\n",
    "\n",
    "            # Emotion detection with DeepFace\n",
    "            emotion_analysis = DeepFace.analyze(face_img, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "            # Extract the dominant emotion, its score and the color associated to it\n",
    "            dominant_emotion = emotion_analysis[0]['dominant_emotion']\n",
    "            emotion_score = emotion_analysis[0]['emotion'][dominant_emotion]\n",
    "            emotion_color = emotion_colors.get(dominant_emotion.lower())\n",
    "\n",
    "            # Save data of the face\n",
    "            face_data.append({\n",
    "                \"box\": (x, y, w, h),\n",
    "                \"emotion\": dominant_emotion,\n",
    "                \"score\": emotion_score,\n",
    "                \"color\": emotion_color\n",
    "                })\n",
    "\n",
    "    #Draw rectangles and text on the frame\n",
    "    for data in face_data:\n",
    "        x, y, w, h = data[\"box\"]\n",
    "        emotion = data[\"emotion\"]\n",
    "        score = data[\"score\"]\n",
    "        color = data[\"color\"]\n",
    "\n",
    "        # Add a background rectangle for the text\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1.2\n",
    "        thickness = 2\n",
    "\n",
    "        # Compute the size of the text to add a background rectangle\n",
    "        (text_width, text_height), _ = cv2.getTextSize(f'{dominant_emotion}: {emotion_score:.0f}', font, font_scale, thickness)\n",
    "\n",
    "        # Create a rectangle for the text\n",
    "        cv2.rectangle(frame, (x, y - 35), (x + text_width + 30, y), color, -1)\n",
    "\n",
    "        # Write the text on the rectangle\n",
    "        cv2.putText(frame, f'{emotion}: {score:.0f}%', (x, y - 5), font, font_scale, (255, 255, 255), thickness)\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "        # Draw circles on the corners of the rectangle\n",
    "        radius = 7\n",
    "        cv2.circle(frame, (x, y), radius, color, -1)  # Top-left corner\n",
    "        cv2.circle(frame, (x + w, y), radius, color, -1)  # Top-right corner\n",
    "        cv2.circle(frame, (x, y + h), radius, color, -1)  # Bottom-left corner\n",
    "        cv2.circle(frame, (x + w, y + h), radius, color, -1)  # Bottom-right corner\n",
    "\n",
    "    \n",
    "    # Show the frame in real time \n",
    "    cv2.imshow(\"Emotion Recognition - Live\", np.hstack((frame, flow_frame)))\n",
    "\n",
    "    # Quit the loop if the user presses 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Increase the frame counter\n",
    "    frame_count += 1\n",
    "\n",
    "# Release the webcam and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_sivpj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
